{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.pipeline import Pipeline\n",
    "import torch\n",
    "from torchdrug import utils, data\n",
    "import pandas as pd\n",
    "\n",
    "GPU = 1\n",
    "\n",
    "def create_single_pred_dataframe(pipeline, dataset):\n",
    "    df = pd.DataFrame()\n",
    "    pipeline.task.eval()\n",
    "    for protein_index, batch in enumerate(data.DataLoader(dataset, batch_size=1, shuffle=False)):\n",
    "        batch = utils.cuda(batch, device=f'cuda:{GPU}')\n",
    "        label = pipeline.task.target(batch)['label'].flatten()\n",
    "        \n",
    "        new_data = {\n",
    "            'protein_index': protein_index,\n",
    "            'residue_index': list(range(len(label))),\n",
    "            'target': label.tolist(),\n",
    "        }\n",
    "        pred = pipeline.task.predict(batch).flatten()\n",
    "        assert(len(label) == len(pred))\n",
    "        new_data[f'pred'] = [round(t, 5) for t in pred.tolist()]\n",
    "        new_data = pd.DataFrame(new_data)\n",
    "        df = pd.concat([df, new_data])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def adaboost_iter(iter_num, masks=None):\n",
    "    # initialize new pipeline\n",
    "    print('Initializing new pipeline')\n",
    "    model_kwargs = {\n",
    "        'gpu': GPU,\n",
    "        'lm_type': 'esm-t33',\n",
    "        'gearnet_hidden_dim_size': 512,\n",
    "        'gearnet_hidden_dim_count': 4,\n",
    "    }\n",
    "    pipeline = Pipeline(\n",
    "        model='lm-gearnet',\n",
    "        dataset='atpbind3d',\n",
    "        gpus=[GPU],\n",
    "        model_kwargs=model_kwargs,\n",
    "        optimizer_kwargs={\n",
    "            'lr': 1e-3,\n",
    "        },\n",
    "        batch_size=4,\n",
    "    )\n",
    "    pipeline.model.freeze_lm(freeze_all=False, freeze_layer_count=30)\n",
    "    \n",
    "    print('Training..')\n",
    "    train_record, state_dict = pipeline.train_until_fit(patience=5, return_state_dict=True, use_dynamic_threshold=False)\n",
    "\n",
    "    print('Train Done')\n",
    "    train_dataloader = data.DataLoader(pipeline.train_set, batch_size=1, shuffle=False)\n",
    "\n",
    "    # load the best model\n",
    "    print('Loading best model')\n",
    "    pipeline.task.load_state_dict(state_dict)\n",
    "    pipeline.task.eval()\n",
    "\n",
    "\n",
    "    # Get the prediction of all residues with negative labels\n",
    "    print('Getting prediciton for negative labels')\n",
    "    if not masks:\n",
    "        masks = [\n",
    "            torch.ones(train_data['graph'].num_residue.item()).bool() \n",
    "            for train_data in pipeline.train_set\n",
    "        ]\n",
    "\n",
    "    negative_labels = []\n",
    "    for protein_index, batch in enumerate(train_dataloader):\n",
    "        batch = utils.cuda(batch, device=f'cuda:{GPU}')\n",
    "        label = pipeline.task.target(batch)['label'].flatten()\n",
    "        pred = pipeline.task.predict(batch).flatten()\n",
    "        for i in range(len(label)):\n",
    "            if label[i] == 0 and masks[protein_index][i]:\n",
    "                negative_labels.append({\n",
    "                    \"protein_index\": protein_index,\n",
    "                    'resudie_index': i,\n",
    "                    'pred': pred[i].item(),\n",
    "                })\n",
    "            \n",
    "    negative_labels = sorted(negative_labels, key=lambda x: x['pred'], reverse=False)\n",
    "    top_10_percent = int(len(negative_labels) * 0.1)\n",
    "    for elem in negative_labels[:top_10_percent]:\n",
    "        masks[elem['protein_index']][elem['resudie_index']] = False\n",
    "\n",
    "    # save prediction of current round\n",
    "    print('Saving prediction')\n",
    "    df_valid = create_single_pred_dataframe(pipeline, pipeline.valid_set)\n",
    "    df_valid.to_csv(f'preds/adaboost_{iter_num:02d}_valid.csv', index=False)\n",
    "\n",
    "    df_test = create_single_pred_dataframe(pipeline, pipeline.test_set)\n",
    "    df_test.to_csv(f'preds/adaboost_{iter_num:02d}_test.csv', index=False)\n",
    "    \n",
    "    return masks\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing new pipeline\n",
      "load model lm-gearnet, kwargs: {'gpu': 1, 'lm_type': 'esm-t33', 'gearnet_hidden_dim_size': 512, 'gearnet_hidden_dim_count': 4}\n",
      "get dataset atpbind3d\n",
      "Initialize RUS: None\n",
      "train samples: 302, valid samples: 76, test samples: 41\n",
      "Training..\n",
      "0m48s {'sensitivity': 0.2695, 'specificity': 0.9966, 'accuracy': 0.9589, 'precision': 0.8125, 'mcc': 0.4539, 'micro_auroc': 0.9095, 'train_bce': 0.1841, 'valid_bce': 0.1192, 'valid_mcc': 0.4189}\n",
      "0m46s {'sensitivity': 0.5981, 'specificity': 0.986, 'accuracy': 0.9659, 'precision': 0.6996, 'mcc': 0.6292, 'micro_auroc': 0.9487, 'train_bce': 0.0893, 'valid_bce': 0.0942, 'valid_mcc': 0.5938}\n",
      "0m46s {'sensitivity': 0.4338, 'specificity': 0.9963, 'accuracy': 0.9671, 'precision': 0.8635, 'mcc': 0.5987, 'micro_auroc': 0.9233, 'train_bce': 0.0522, 'valid_bce': 0.1243, 'valid_mcc': 0.584}\n",
      "0m46s {'sensitivity': 0.4689, 'specificity': 0.9933, 'accuracy': 0.9661, 'precision': 0.7925, 'mcc': 0.5943, 'micro_auroc': 0.92, 'train_bce': 0.0324, 'valid_bce': 0.1329, 'valid_mcc': 0.5813}\n",
      "0m47s {'sensitivity': 0.6826, 'specificity': 0.9766, 'accuracy': 0.9613, 'precision': 0.6141, 'mcc': 0.6271, 'micro_auroc': 0.9277, 'train_bce': 0.0195, 'valid_bce': 0.13, 'valid_mcc': 0.629}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m masks \u001b[39m=\u001b[39m adaboost_iter(iter_num\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, masks\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[1], line 50\u001b[0m, in \u001b[0;36madaboost_iter\u001b[0;34m(iter_num, masks)\u001b[0m\n\u001b[1;32m     47\u001b[0m pipeline\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mfreeze_lm(freeze_all\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, freeze_layer_count\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTraining..\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m train_record, state_dict \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mtrain_until_fit(patience\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, return_state_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, use_dynamic_threshold\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     52\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTrain Done\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     53\u001b[0m train_dataloader \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mDataLoader(pipeline\u001b[39m.\u001b[39mtrain_set, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/atpbind/lib/pipeline.py:190\u001b[0m, in \u001b[0;36mPipeline.train_until_fit\u001b[0;34m(self, patience, early_stop_metric, return_state_dict, use_dynamic_threshold)\u001b[0m\n\u001b[1;32m    186\u001b[0m     cur_result[\u001b[39m'\u001b[39m\u001b[39mvalid_mcc\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculate_best_mcc_and_threshold(\n\u001b[1;32m    187\u001b[0m         threshold_set\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    188\u001b[0m     )[\u001b[39m'\u001b[39m\u001b[39mbest_mcc\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     cur_result[\u001b[39m'\u001b[39m\u001b[39mvalid_mcc\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mvalid\u001b[39;49m\u001b[39m'\u001b[39;49m, threshold\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)[\u001b[39m'\u001b[39m\u001b[39mmcc\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    191\u001b[0m cur_result \u001b[39m=\u001b[39m round_dict(cur_result, \u001b[39m4\u001b[39m)\n\u001b[1;32m    192\u001b[0m train_record\u001b[39m.\u001b[39mappend(cur_result)\n",
      "File \u001b[0;32m~/atpbind/lib/pipeline.py:281\u001b[0m, in \u001b[0;36mPipeline.evaluate\u001b[0;34m(self, split, verbose, threshold)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    280\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask\u001b[39m.\u001b[39mthreshold \u001b[39m=\u001b[39m threshold\n\u001b[0;32m--> 281\u001b[0m \u001b[39mreturn\u001b[39;00m dict_tensor_to_num(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msolver\u001b[39m.\u001b[39;49mevaluate(split\u001b[39m=\u001b[39;49msplit))\n",
      "File \u001b[0;32m~/miniconda3/envs/jc/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/jc/lib/python3.9/site-packages/torchdrug/core/engine.py:206\u001b[0m, in \u001b[0;36mEngine.evaluate\u001b[0;34m(self, split, log)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    204\u001b[0m     batch \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mcuda(batch, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 206\u001b[0m pred, target \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict_and_target(batch)\n\u001b[1;32m    207\u001b[0m preds\u001b[39m.\u001b[39mappend(pred)\n\u001b[1;32m    208\u001b[0m targets\u001b[39m.\u001b[39mappend(target)\n",
      "File \u001b[0;32m~/miniconda3/envs/jc/lib/python3.9/site-packages/torchdrug/tasks/task.py:30\u001b[0m, in \u001b[0;36mTask.predict_and_target\u001b[0;34m(self, batch, all_loss, metric)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_and_target\u001b[39m(\u001b[39mself\u001b[39m, batch, all_loss\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, metric\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(batch, all_loss, metric), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget(batch)\n",
      "File \u001b[0;32m~/atpbind/lib/tasks.py:71\u001b[0m, in \u001b[0;36mNodePropertyPrediction.predict\u001b[0;34m(self, batch, all_loss, metric)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39msparse\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m node_feature\u001b[39m.\u001b[39mtype():\n\u001b[1;32m     70\u001b[0m     node_feature \u001b[39m=\u001b[39m node_feature\u001b[39m.\u001b[39mto_dense()\n\u001b[0;32m---> 71\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(graph, node_feature, all_loss\u001b[39m=\u001b[39;49mall_loss, metric\u001b[39m=\u001b[39;49mmetric)\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mview \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mnode\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39matom\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     73\u001b[0m     output_feature \u001b[39m=\u001b[39m output[\u001b[39m\"\u001b[39m\u001b[39mnode_feature\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/jc/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/atpbind/lib/custom_models.py:71\u001b[0m, in \u001b[0;36mLMGearNetModel.forward\u001b[0;34m(self, graph, _, all_loss, metric)\u001b[0m\n\u001b[1;32m     67\u001b[0m     lm_residue_feature\u001b[39m.\u001b[39mappend(emb[\u001b[39m1\u001b[39m:\u001b[39m1\u001b[39m\u001b[39m+\u001b[39minput_len[i]])\n\u001b[1;32m     69\u001b[0m lm_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(lm_residue_feature)\n\u001b[0;32m---> 71\u001b[0m gearnet_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgearnet(graph, lm_output)\n\u001b[1;32m     73\u001b[0m final_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([gearnet_output[\u001b[39m'\u001b[39m\u001b[39mnode_feature\u001b[39m\u001b[39m'\u001b[39m], lm_output], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_concat_to_output \u001b[39melse\u001b[39;00m gearnet_output[\u001b[39m'\u001b[39m\u001b[39mnode_feature\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_short_cut:\n",
      "File \u001b[0;32m~/miniconda3/envs/jc/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/jc/lib/python3.9/site-packages/torchdrug/models/gearnet.py:98\u001b[0m, in \u001b[0;36mGeometryAwareRelationalGraphNeuralNetwork.forward\u001b[0;34m(self, graph, input, all_loss, metric)\u001b[0m\n\u001b[1;32m     96\u001b[0m     hidden \u001b[39m=\u001b[39m hidden \u001b[39m+\u001b[39m layer_input\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_angle_bin:\n\u001b[0;32m---> 98\u001b[0m     edge_hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49medge_layers[i](line_graph, edge_input)\n\u001b[1;32m     99\u001b[0m     edge_weight \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39medge_weight\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    100\u001b[0m     node_out \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39medge_list[:, \u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_relation \u001b[39m+\u001b[39m graph\u001b[39m.\u001b[39medge_list[:, \u001b[39m2\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/jc/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/jc/lib/python3.9/site-packages/torchdrug/layers/conv.py:91\u001b[0m, in \u001b[0;36mMessagePassingBase.forward\u001b[0;34m(self, graph, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     update \u001b[39m=\u001b[39m checkpoint\u001b[39m.\u001b[39mcheckpoint(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_and_aggregate, \u001b[39m*\u001b[39mgraph\u001b[39m.\u001b[39mto_tensors(), \u001b[39minput\u001b[39m)\n\u001b[1;32m     90\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     update \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmessage_and_aggregate(graph, \u001b[39minput\u001b[39;49m)\n\u001b[1;32m     92\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombine(\u001b[39minput\u001b[39m, update)\n\u001b[1;32m     93\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/jc/lib/python3.9/site-packages/torchdrug/layers/conv.py:804\u001b[0m, in \u001b[0;36mGeometricRelationalGraphConv.message_and_aggregate\u001b[0;34m(self, graph, input)\u001b[0m\n\u001b[1;32m    801\u001b[0m node_out \u001b[39m=\u001b[39m node_out \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_relation \u001b[39m+\u001b[39m relation\n\u001b[1;32m    802\u001b[0m adjacency \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39msparse_coo_tensor(torch\u001b[39m.\u001b[39mstack([node_in, node_out]), graph\u001b[39m.\u001b[39medge_weight,\n\u001b[1;32m    803\u001b[0m                                     (graph\u001b[39m.\u001b[39mnum_node, graph\u001b[39m.\u001b[39mnum_node \u001b[39m*\u001b[39m graph\u001b[39m.\u001b[39mnum_relation))\n\u001b[0;32m--> 804\u001b[0m update \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49msparse\u001b[39m.\u001b[39;49mmm(adjacency\u001b[39m.\u001b[39;49mt(), \u001b[39minput\u001b[39;49m)\n\u001b[1;32m    805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39medge_linear:\n\u001b[1;32m    806\u001b[0m     edge_input \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39medge_feature\u001b[39m.\u001b[39mfloat()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "masks = adaboost_iter(iter_num=1, masks=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
