{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdrug import transforms\n",
    "from torchdrug import data, core, layers, tasks, metrics, utils, models\n",
    "from torchdrug.layers import functional\n",
    "from torchdrug.core import Registry as R\n",
    "\n",
    "import torch\n",
    "from torch.utils import data as torch_data\n",
    "from torch.nn import functional as F\n",
    "from lib.tasks import NodePropertyPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split num:  [346, 42, 41]\n",
      "train samples: 346, valid samples: 42, test samples: 41\n"
     ]
    }
   ],
   "source": [
    "from lib.datasets import ATPBind\n",
    "\n",
    "truncuate_transform = transforms.TruncateProtein(max_length=350, random=False)\n",
    "protein_view_transform = transforms.ProteinView(view='residue')\n",
    "transform = transforms.Compose([truncuate_transform, protein_view_transform])\n",
    "\n",
    "dataset = ATPBind(atom_feature=None, bond_feature=None,\n",
    "                  residue_feature=\"default\", transform=transform)\n",
    "\n",
    "train_set, valid_set, test_set = dataset.split()\n",
    "print(\"train samples: %d, valid samples: %d, test samples: %d\" %\n",
    "      (len(train_set), len(valid_set), len(test_set)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze_bert in https://github.com/aws-samples/lm-gvp/blob/0b7a6d96486e2ee222929917570432296554cfe7/lmgvp/modules.py#L47\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "def _freeze_bert(\n",
    "    bert_model: BertModel, freeze_bert=True, freeze_layer_count=-1\n",
    "):\n",
    "    \"\"\"Freeze parameters in BertModel (in place)\n",
    "\n",
    "    Args:\n",
    "        bert_model: HuggingFace bert model\n",
    "        freeze_bert: Bool whether or not to freeze the bert model\n",
    "        freeze_layer_count: If freeze_bert, up to what layer to freeze.\n",
    "\n",
    "    Returns:\n",
    "        bert_model\n",
    "    \"\"\"\n",
    "    if freeze_bert:\n",
    "        # freeze the entire bert model\n",
    "        for param in bert_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        # freeze the embeddings\n",
    "        for param in bert_model.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "        if freeze_layer_count != -1:\n",
    "            # freeze layers in bert_model.encoder\n",
    "            for layer in bert_model.encoder.layer[:freeze_layer_count]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cusom model Wrapping BERT: check https://torchdrug.ai/docs/notes/model.html\n",
    "class BertWrapModel(torch.nn.Module, core.Configurable):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "            \"Rostlab/prot_bert\", do_lower_case=False)\n",
    "        self.bert_model = BertModel.from_pretrained(\n",
    "            \"Rostlab/prot_bert\").to('cuda')\n",
    "        _freeze_bert(self.bert_model, freeze_bert=False, freeze_layer_count=29)\n",
    "        self.input_dim = 21\n",
    "        self.output_dim = self.bert_model.config.hidden_size\n",
    "\n",
    "    def forward(self, graph, _, all_loss=None, metric=None):\n",
    "        # print(\"graph: \", graph)\n",
    "        # print(\"sequence: \", graph.to_sequence())\n",
    "        input = [seq.replace('.', ' ') for seq in graph.to_sequence()]\n",
    "\n",
    "        encoded_input = self.bert_tokenizer(\n",
    "            input, return_tensors='pt').to('cuda')\n",
    "        # print(\"Input size: \", encoded_input[\"input_ids\"].size())\n",
    "        x = self.bert_model(**encoded_input)\n",
    "        # print(\"Output size just after model: \", x.last_hidden_state.size())\n",
    "        \n",
    "        # skip residue feature for [CLS] and [SEP], since they are not in the original sequence\n",
    "        return {\"residue_feature\": torch.squeeze(x.last_hidden_state)[1:-1]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:32:26   Preprocess training set\n",
      "11:32:32   {'batch_size': 1,\n",
      " 'class': 'core.Engine',\n",
      " 'gpus': [2],\n",
      " 'gradient_interval': 1,\n",
      " 'log_interval': 1000,\n",
      " 'logger': 'logging',\n",
      " 'num_worker': 0,\n",
      " 'optimizer': {'amsgrad': False,\n",
      "               'betas': (0.9, 0.999),\n",
      "               'class': 'optim.Adam',\n",
      "               'eps': 1e-08,\n",
      "               'lr': 0.001,\n",
      "               'weight_decay': 0},\n",
      " 'scheduler': None,\n",
      " 'task': {'class': 'NodePropertyPrediction',\n",
      "          'criterion': 'bce',\n",
      "          'metric': ('micro_auroc',\n",
      "                     'micro_auprc',\n",
      "                     'macro_auprc',\n",
      "                     'macro_auroc'),\n",
      "          'model': {'class': 'BertWrapModel'},\n",
      "          'normalization': False,\n",
      "          'num_class': None,\n",
      "          'num_mlp_layer': 2,\n",
      "          'verbose': 0},\n",
      " 'test_set': {'class': 'dataset.Subset',\n",
      "              'dataset': {'atom_feature': None,\n",
      "                          'bond_feature': None,\n",
      "                          'class': 'ATPBind',\n",
      "                          'path': None,\n",
      "                          'residue_feature': 'default',\n",
      "                          'transform': {'class': 'transforms.Compose',\n",
      "                                        'transforms': [<torchdrug.transforms.transform.TruncateProtein object at 0x7fec507a5d60>,\n",
      "                                                       <torchdrug.transforms.transform.ProteinView object at 0x7fec507a5490>]},\n",
      "                          'valid_ratio': 0.1,\n",
      "                          'verbose': 1},\n",
      "              'indices': range(388, 429)},\n",
      " 'train_set': {'class': 'dataset.Subset',\n",
      "               'dataset': {'atom_feature': None,\n",
      "                           'bond_feature': None,\n",
      "                           'class': 'ATPBind',\n",
      "                           'path': None,\n",
      "                           'residue_feature': 'default',\n",
      "                           'transform': {'class': 'transforms.Compose',\n",
      "                                         'transforms': [<torchdrug.transforms.transform.TruncateProtein object at 0x7fec507a5d60>,\n",
      "                                                        <torchdrug.transforms.transform.ProteinView object at 0x7fec507a5490>]},\n",
      "                           'valid_ratio': 0.1,\n",
      "                           'verbose': 1},\n",
      "               'indices': range(0, 346)},\n",
      " 'valid_set': {'class': 'dataset.Subset',\n",
      "               'dataset': {'atom_feature': None,\n",
      "                           'bond_feature': None,\n",
      "                           'class': 'ATPBind',\n",
      "                           'path': None,\n",
      "                           'residue_feature': 'default',\n",
      "                           'transform': {'class': 'transforms.Compose',\n",
      "                                         'transforms': [<torchdrug.transforms.transform.TruncateProtein object at 0x7fec507a5d60>,\n",
      "                                                        <torchdrug.transforms.transform.ProteinView object at 0x7fec507a5490>]},\n",
      "                           'valid_ratio': 0.1,\n",
      "                           'verbose': 1},\n",
      "               'indices': range(346, 388)}}\n"
     ]
    }
   ],
   "source": [
    "bert_wrap_model = BertWrapModel()\n",
    "bert_task = NodePropertyPrediction(\n",
    "    bert_wrap_model, \n",
    "    normalization=False,\n",
    "    num_mlp_layer=2,\n",
    "    metric=(\"micro_auroc\", \"micro_auprc\", \"macro_auprc\", \"macro_auroc\")\n",
    ")\n",
    "optimizer = torch.optim.Adam(bert_task.parameters(), lr=1e-3)\n",
    "solver = core.Engine(bert_task, train_set, valid_set, test_set, optimizer, batch_size=1, log_interval=1000, gpus=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:32:32   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "11:32:32   Epoch 0 begin\n",
      "11:32:32   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "11:32:32   binary cross entropy: 0.680496\n",
      "11:32:54   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "11:32:54   Epoch 0 end\n",
      "11:32:54   duration: 22.20 secs\n",
      "11:32:54   speed: 15.59 batch / sec\n",
      "11:32:54   ETA: 0.00 secs\n",
      "11:32:54   max GPU memory: 1911.9 MiB\n",
      "11:32:54   ------------------------------\n",
      "11:32:54   average binary cross entropy: 0.160703\n",
      "11:32:54   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "11:32:54   Evaluate on valid\n",
      "11:32:56   ------------------------------\n",
      "11:32:56   macro_auprc: 0.450954\n",
      "11:32:56   macro_auroc: 0.869341\n",
      "11:32:56   micro_auprc: 0.481848\n",
      "11:32:56   micro_auroc: 0.899687\n",
      "11:32:56   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "11:32:56   Evaluate on test\n",
      "11:32:58   ------------------------------\n",
      "11:32:58   macro_auprc: 0.505588\n",
      "11:32:58   macro_auroc: 0.880655\n",
      "11:32:58   micro_auprc: 0.49297\n",
      "11:32:58   micro_auroc: 0.894856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'micro_auroc': tensor(0.8949, device='cuda:2'),\n",
       " 'micro_auprc': tensor(0.4930, device='cuda:2'),\n",
       " 'macro_auprc': tensor(0.5056, device='cuda:2'),\n",
       " 'macro_auroc': tensor(0.8807, device='cuda:2')}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver.train(num_epoch=1)\n",
    "solver.evaluate(\"valid\")\n",
    "solver.evaluate(\"test\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jc",
   "language": "python",
   "name": "jc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
