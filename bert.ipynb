{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/project/aigenintern/aigenintern2/miniconda3/envs/jc/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchdrug import transforms\n",
    "from torchdrug import data, core, layers, tasks, metrics, utils, models\n",
    "from torchdrug.layers import functional\n",
    "from torchdrug.core import Registry as R\n",
    "\n",
    "import torch\n",
    "from torch.utils import data as torch_data\n",
    "from torch.nn import functional as F\n",
    "from lib.tasks import NodePropertyPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split num:  [346, 42, 41]\n",
      "train samples: 346, valid samples: 42, test samples: 41\n"
     ]
    }
   ],
   "source": [
    "from lib.datasets import ATPBind\n",
    "\n",
    "truncuate_transform = transforms.TruncateProtein(max_length=350, random=False)\n",
    "protein_view_transform = transforms.ProteinView(view='residue')\n",
    "transform = transforms.Compose([truncuate_transform, protein_view_transform])\n",
    "\n",
    "dataset = ATPBind(atom_feature=None, bond_feature=None,\n",
    "                  residue_feature=\"default\", transform=transform)\n",
    "\n",
    "train_set, valid_set, test_set = dataset.split()\n",
    "print(\"train samples: %d, valid samples: %d, test samples: %d\" %\n",
    "      (len(train_set), len(valid_set), len(test_set)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze_bert in https://github.com/aws-samples/lm-gvp/blob/0b7a6d96486e2ee222929917570432296554cfe7/lmgvp/modules.py#L47\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "def _freeze_bert(\n",
    "    bert_model: BertModel, freeze_bert=True, freeze_layer_count=-1\n",
    "):\n",
    "    \"\"\"Freeze parameters in BertModel (in place)\n",
    "\n",
    "    Args:\n",
    "        bert_model: HuggingFace bert model\n",
    "        freeze_bert: Bool whether or not to freeze the bert model\n",
    "        freeze_layer_count: If freeze_bert, up to what layer to freeze.\n",
    "\n",
    "    Returns:\n",
    "        bert_model\n",
    "    \"\"\"\n",
    "    if freeze_bert:\n",
    "        # freeze the entire bert model\n",
    "        for param in bert_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        # freeze the embeddings\n",
    "        for param in bert_model.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "        if freeze_layer_count != -1:\n",
    "            # freeze layers in bert_model.encoder\n",
    "            for layer in bert_model.encoder.layer[:freeze_layer_count]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "glob_bert_tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "glob_bert_model = BertModel.from_pretrained(\"Rostlab/prot_bert\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cusom model Wrapping BERT: check https://torchdrug.ai/docs/notes/model.html\n",
    "class BertWrapModel(torch.nn.Module, core.Configurable):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert_tokenizer = glob_bert_tokenizer\n",
    "        self.bert_model = glob_bert_model\n",
    "        _freeze_bert(self.bert_model, freeze_bert=False, freeze_layer_count=-1)\n",
    "        self.input_dim = 21\n",
    "        self.output_dim = self.bert_model.config.hidden_size\n",
    "\n",
    "    def forward(self, graph, _, all_loss=None, metric=None):\n",
    "        # print(\"graph: \", graph)\n",
    "        # print(\"sequence: \", graph.to_sequence())\n",
    "        input = [seq.replace('.', ' ') for seq in graph.to_sequence()]\n",
    "\n",
    "        encoded_input = self.bert_tokenizer(\n",
    "            input, return_tensors='pt').to('cuda')\n",
    "        # print(\"Input size: \", encoded_input[\"input_ids\"].size())\n",
    "        x = self.bert_model(**encoded_input)\n",
    "        # print(\"Output size just after model: \", x.last_hidden_state.size())\n",
    "        \n",
    "        # Since tokenizer adds start and end token, need to remove these hidden states\n",
    "        return {\"residue_feature\": torch.squeeze(x.last_hidden_state)[1:-1]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:07:01   Preprocess training set\n",
      "14:07:01   {'batch_size': 1,\n",
      " 'class': 'core.Engine',\n",
      " 'gpus': [0],\n",
      " 'gradient_interval': 1,\n",
      " 'log_interval': 1000,\n",
      " 'logger': 'logging',\n",
      " 'num_worker': 0,\n",
      " 'optimizer': {'amsgrad': False,\n",
      "               'betas': (0.9, 0.999),\n",
      "               'class': 'optim.Adam',\n",
      "               'eps': 1e-08,\n",
      "               'lr': 0.001,\n",
      "               'weight_decay': 0},\n",
      " 'scheduler': None,\n",
      " 'task': {'class': 'NodePropertyPrediction',\n",
      "          'criterion': 'bce',\n",
      "          'metric': ('micro_auroc',\n",
      "                     'micro_auprc',\n",
      "                     'macro_auprc',\n",
      "                     'macro_auroc'),\n",
      "          'model': {'class': 'BertWrapModel'},\n",
      "          'normalization': False,\n",
      "          'num_class': None,\n",
      "          'num_mlp_layer': 2,\n",
      "          'verbose': 0},\n",
      " 'test_set': {'class': 'dataset.Subset',\n",
      "              'dataset': {'atom_feature': None,\n",
      "                          'bond_feature': None,\n",
      "                          'class': 'ATPBind',\n",
      "                          'path': None,\n",
      "                          'residue_feature': 'default',\n",
      "                          'transform': {'class': 'transforms.Compose',\n",
      "                                        'transforms': [<torchdrug.transforms.transform.TruncateProtein object at 0x7fa4e9cc7ee0>,\n",
      "                                                       <torchdrug.transforms.transform.ProteinView object at 0x7fa4e9cc75b0>]},\n",
      "                          'valid_ratio': 0.1,\n",
      "                          'verbose': 1},\n",
      "              'indices': range(388, 429)},\n",
      " 'train_set': {'class': 'dataset.Subset',\n",
      "               'dataset': {'atom_feature': None,\n",
      "                           'bond_feature': None,\n",
      "                           'class': 'ATPBind',\n",
      "                           'path': None,\n",
      "                           'residue_feature': 'default',\n",
      "                           'transform': {'class': 'transforms.Compose',\n",
      "                                         'transforms': [<torchdrug.transforms.transform.TruncateProtein object at 0x7fa4e9cc7ee0>,\n",
      "                                                        <torchdrug.transforms.transform.ProteinView object at 0x7fa4e9cc75b0>]},\n",
      "                           'valid_ratio': 0.1,\n",
      "                           'verbose': 1},\n",
      "               'indices': range(0, 346)},\n",
      " 'valid_set': {'class': 'dataset.Subset',\n",
      "               'dataset': {'atom_feature': None,\n",
      "                           'bond_feature': None,\n",
      "                           'class': 'ATPBind',\n",
      "                           'path': None,\n",
      "                           'residue_feature': 'default',\n",
      "                           'transform': {'class': 'transforms.Compose',\n",
      "                                         'transforms': [<torchdrug.transforms.transform.TruncateProtein object at 0x7fa4e9cc7ee0>,\n",
      "                                                        <torchdrug.transforms.transform.ProteinView object at 0x7fa4e9cc75b0>]},\n",
      "                           'valid_ratio': 0.1,\n",
      "                           'verbose': 1},\n",
      "               'indices': range(346, 388)}}\n"
     ]
    }
   ],
   "source": [
    "bert_wrap_model = BertWrapModel()\n",
    "bert_task = NodePropertyPrediction(\n",
    "    bert_wrap_model, \n",
    "    normalization=False,\n",
    "    num_mlp_layer=2,\n",
    "    metric=(\"micro_auroc\", \"micro_auprc\", \"macro_auprc\", \"macro_auroc\")\n",
    ")\n",
    "optimizer = torch.optim.Adam(bert_task.parameters(), lr=1e-3)\n",
    "solver = core.Engine(bert_task, train_set, valid_set, test_set, optimizer, batch_size=1, log_interval=1000, gpus=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:08:13   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "14:08:13   Epoch 0 begin\n",
      "14:09:41   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "14:09:41   Epoch 0 end\n",
      "14:09:41   duration: 2.67 mins\n",
      "14:09:41   speed: 3.83 batch / sec\n",
      "14:09:41   ETA: 0.00 secs\n",
      "14:09:41   max GPU memory: 7642.8 MiB\n",
      "14:09:41   ------------------------------\n",
      "14:09:41   average binary cross entropy: 0.206939\n",
      "14:09:41   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "14:09:41   Evaluate on valid\n",
      "14:09:44   ------------------------------\n",
      "14:09:44   macro_auprc: 0.0716508\n",
      "14:09:44   macro_auroc: 0.539814\n",
      "14:09:44   micro_auprc: 0.0401717\n",
      "14:09:44   micro_auroc: 0.501429\n",
      "14:09:44   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "14:09:44   Evaluate on test\n",
      "14:09:46   ------------------------------\n",
      "14:09:46   macro_auprc: 0.0797101\n",
      "14:09:46   macro_auroc: 0.529725\n",
      "14:09:46   micro_auprc: 0.050877\n",
      "14:09:46   micro_auroc: 0.49499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'micro_auroc': tensor(0.4950, device='cuda:0'),\n",
       " 'micro_auprc': tensor(0.0509, device='cuda:0'),\n",
       " 'macro_auprc': tensor(0.0797, device='cuda:0'),\n",
       " 'macro_auroc': tensor(0.5297, device='cuda:0')}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver.train(num_epoch=1)\n",
    "solver.evaluate(\"valid\")\n",
    "solver.evaluate(\"test\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jc",
   "language": "python",
   "name": "jc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
