{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.datasets import ATPBind3D\n",
    "\n",
    "from torchdrug import transforms\n",
    "from torchdrug import data, core, layers, tasks, metrics, utils, models\n",
    "from torchdrug.layers import functional\n",
    "from torchdrug.core import Registry as R\n",
    "\n",
    "import torch\n",
    "from torch.utils import data as torch_data\n",
    "from torch.nn import functional as F\n",
    "from lib.tasks import NodePropertyPrediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split num:  [337, 41, 41]\n",
      "train samples: 337, valid samples: 41, test samples: 41\n"
     ]
    }
   ],
   "source": [
    "truncuate_transform = transforms.TruncateProtein(max_length=350, random=False)\n",
    "protein_view_transform = transforms.ProteinView(view='residue')\n",
    "transform = transforms.Compose([truncuate_transform, protein_view_transform])\n",
    "\n",
    "dataset = ATPBind3D(transform=transform)\n",
    "\n",
    "train_set, valid_set, test_set = dataset.split()\n",
    "print(\"train samples: %d, valid samples: %d, test samples: %d\" %\n",
    "      (len(train_set), len(valid_set), len(test_set)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "bert_model_global = BertModel.from_pretrained(\"Rostlab/prot_bert\").to('cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "\n",
    "def _freeze_bert(\n",
    "    bert_model: BertModel, freeze_bert=True, freeze_layer_count=-1\n",
    "):\n",
    "    \"\"\"Freeze parameters in BertModel (in place)\n",
    "\n",
    "    Args:\n",
    "        bert_model: HuggingFace bert model\n",
    "        freeze_bert: Bool whether or not to freeze the bert model\n",
    "        freeze_layer_count: If freeze_bert, up to what layer to freeze.\n",
    "\n",
    "    Returns:\n",
    "        bert_model\n",
    "    \"\"\"\n",
    "    if freeze_bert:\n",
    "        # freeze the entire bert model\n",
    "        for param in bert_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        # freeze the embeddings\n",
    "        for param in bert_model.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "        if freeze_layer_count != -1:\n",
    "            # freeze layers in bert_model.encoder\n",
    "            for layer in bert_model.encoder.layer[:freeze_layer_count]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "    return None\n",
    "\n",
    "\n",
    "def separate_alphabets(text):\n",
    "    separated_text = \"\"\n",
    "    for char in text:\n",
    "        if char.isalpha():\n",
    "            separated_text += char + \" \"\n",
    "    return separated_text.strip()\n",
    "\n",
    "class LMGearNetModel(torch.nn.Module, core.Configurable):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "            \"Rostlab/prot_bert\", do_lower_case=False)\n",
    "        self.bert_model = bert_model_global\n",
    "        _freeze_bert(self.bert_model, freeze_bert=False, freeze_layer_count=29)\n",
    "        self.gearnet = models.GearNet(\n",
    "            input_dim=1024, #self.bert_model.config.hidden_size,\n",
    "            hidden_dims=[512, 512, 512, 512, 512, 512],\n",
    "            num_relation=7,\n",
    "            edge_input_dim=59,\n",
    "            num_angle_bin=8,\n",
    "            batch_norm=True,\n",
    "            concat_hidden=True,\n",
    "            short_cut=True,\n",
    "            readout=\"sum\"\n",
    "        ).to('cuda:2')\n",
    "        self.input_dim = 21\n",
    "        self.output_dim = self.gearnet.output_dim\n",
    "\n",
    "    def forward(self, graph, _, all_loss=None, metric=None):\n",
    "        # print(\"at forward, graph: \", graph)\n",
    "        # print(\"sequence: \", graph.to_sequence())\n",
    "        input = [separate_alphabets(seq) for seq in graph.to_sequence()]\n",
    "\n",
    "        encoded_input = self.bert_tokenizer(\n",
    "            input, return_tensors='pt').to('cuda:2')\n",
    "        # print(\"Input size: \", encoded_input[\"input_ids\"].size())\n",
    "        x = self.bert_model(**encoded_input)\n",
    "        # print(\"Output size just after bert model: \", x.last_hidden_state.size())\n",
    "        \n",
    "        # skip residue feature for [CLS] and [SEP], since they are not in the original sequence\n",
    "        lm_output = x.last_hidden_state.squeeze()[1:-1]\n",
    "        \n",
    "        # print(f'lm_output shape: {lm_output.shape}')\n",
    "        gearnet_output = self.gearnet(graph, lm_output)\n",
    "        return gearnet_output\n",
    "    \n",
    "\n",
    "lm_gearnet = LMGearNetModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdrug import core, layers\n",
    "from torchdrug.layers import geometry\n",
    "import torch\n",
    "from lib.disable_logger import DisableLogger\n",
    "\n",
    "graph_construction_model = layers.GraphConstruction(node_layers=[geometry.AlphaCarbonNode()],\n",
    "                                                    edge_layers=[geometry.SpatialEdge(radius=10.0, min_distance=5),\n",
    "                                                                 geometry.KNNEdge(\n",
    "                                                                     k=10, min_distance=5),\n",
    "                                                                 geometry.SequentialEdge(max_distance=2)],\n",
    "                                                    edge_feature=\"gearnet\")\n",
    "\n",
    "task = NodePropertyPrediction(\n",
    "    lm_gearnet, \n",
    "    normalization=False,\n",
    "    num_mlp_layer=2,\n",
    "    metric=(\"micro_auroc\", \"mcc\"),\n",
    "    graph_construction_model=graph_construction_model,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(task.parameters(), lr=1e-3)\n",
    "with DisableLogger():\n",
    "    solver = core.Engine(task, train_set, valid_set, test_set, optimizer, batch_size=1, log_interval=1000, gpus=[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:21:08   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "21:21:08   Epoch 2 begin\n",
      "21:21:42   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "21:21:42   binary cross entropy: 0.240249\n",
      "21:22:42   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "21:22:42   Epoch 2 end\n",
      "21:22:42   duration: 2.57 mins\n",
      "21:22:42   speed: 2.19 batch / sec\n",
      "21:22:42   ETA: 0.00 secs\n",
      "21:22:42   max GPU memory: 5132.6 MiB\n",
      "21:22:42   ------------------------------\n",
      "21:22:42   average binary cross entropy: 0.0889349\n"
     ]
    }
   ],
   "source": [
    "solver.train(num_epoch=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:25:17   >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "21:25:17   Evaluate on test\n",
      "21:25:25   ------------------------------\n",
      "21:25:25   mcc: 0.446728\n",
      "21:25:25   micro_auroc: 0.883612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'micro_auroc': tensor(0.8836, device='cuda:2'), 'mcc': 0.44672766097239364}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver.evaluate(\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jc",
   "language": "python",
   "name": "jc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
